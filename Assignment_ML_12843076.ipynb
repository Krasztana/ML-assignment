{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment ML 12843076.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDRYELokyZtF"
      },
      "source": [
        "Getting the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uae3anENyYzd"
      },
      "source": [
        "from sklearn.datasets import make_regression\n",
        "\n",
        "from torch import Tensor\n",
        "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "class my_data(Dataset):\n",
        "\n",
        "    def __init__(self):\n",
        "        X, y = make_regression(n_samples=2000, n_features=10, n_informative=6, n_targets=3, random_state=0)\n",
        "\n",
        "        # z-scores for normalising\n",
        "        self.X = (X - np.mean(X, axis=0))/np.std(X, axis=0)\n",
        "        self.y = (y - np.mean(y, axis=0))/np.std(y, axis=0)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return [self.X[idx], self.y[idx]]\n",
        " \n",
        "    def get_splits(self, n_test=0.3):\n",
        "        size_for_test = round(n_test * len(self.X))\n",
        "        size_for_training = len(self.X) - size_for_test\n",
        "        return random_split(self, [size_for_training, size_for_test])\n",
        "\n",
        "\n",
        "dataset = my_data()\n",
        "train, test = dataset.get_splits()\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qJRBTMd69H7"
      },
      "source": [
        "Getting the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHLKt-Nf683y"
      },
      "source": [
        "# multi-output multi-layer perceptron (ANN)\n",
        "class my_MLP():\n",
        "  def __init__(self, X, y):\n",
        "\n",
        "        ''' \n",
        "        for sample size n\n",
        "        X: input data with d features to predict upon   (dimension: (n x d))\n",
        "        y: target data with m outputs to predict        (dimension: (n x m))\n",
        "        h: number of nodes in the hidden layers\n",
        "\n",
        "        learning_rate (alpha): multiplies the gradient update\n",
        "        momentum (mu): multiplies the gradient update carried from the preceding epoch\n",
        "\n",
        "        input layer (size: d for d features)\n",
        "\n",
        "        hidden layers (size: h, which is arbitrary):\n",
        "        w1: weight  (dimension: (d x h))\n",
        "        b1: bias    (dimension: (1 x h))\n",
        "        w2: weight  (dimension: (h x h))\n",
        "        b2: bias    (dimension: (1 x h))\n",
        "        ... additional hidden layers follow w2 and b2\n",
        "        output layer (size: m for m outputs):\n",
        "        w3: weight  (dimension: (h x m))\n",
        "        b3: bias    (dimension: (1 x m))\n",
        "\n",
        "        forward pass:\n",
        "        a1 = x . w1 + b1          (dimension: (n x h))\n",
        "        z1 = ReLU(a1)=max(a1,0)   (dimension: (n x h))\n",
        "        a2 = z1 . w2 + b2         (dimension: (n x h))\n",
        "        z2 = ReLU(a2)=max(a2,0)   (dimension: (n x h))\n",
        "        ... additional hidden layers follow a2 and z2\n",
        "        a3 = z2 . w3 + b3         (dimension: (n x m))\n",
        "        z3 = hTan(a3)=max(a3,0)   (dimension: (n x m))\n",
        "        returns z3\n",
        "\n",
        "        loss function: mean squared error (MSE)\n",
        "        activation function: rectified linear unit (ReLU) and hyperbolic tangent (hTan) for the outer layer\n",
        "\n",
        "        backward pass:\n",
        "        deltaL = dl/dzL * dzL/daL\n",
        "        for j in (L:-1:0):\n",
        "          using the chain rule dl/dwj = dl/dzj * dzj/daj * daj/dwj, with dot products (.) and scalar/piecewise multiplication (*) respectively, gives\n",
        "          wj -= alpha * deltaj . z[j-1]\n",
        "          bj -= alpha * deltaj\n",
        "          delta[j-1]=sigma_gradient(a[j-1])*wj . deltaj\n",
        "        returns the adjusted weights and biases\n",
        "        '''\n",
        "\n",
        "        self.X, d = X, X.shape[1] # d=10 inputs\n",
        "        self.y, m = y, y.shape[1] # m=3 outputs\n",
        "\n",
        "        h = 11\n",
        "        self.learning_rate = 0.01\n",
        "        self.momentum = 0.5\n",
        "        \n",
        "        # the weights (w1,w2,w3) and the stored weights for the momentum are initialised randomly from Gaussian\n",
        "        self.w1, self.w1_stored = np.random.randn(d, h), np.random.randn(d, h)\n",
        "        self.w2, self.w2_stored = np.random.randn(h, h), np.random.randn(h, h)\n",
        "        self.w3, self.w3_stored = np.random.randn(h, m), np.random.randn(h, m)\n",
        "\n",
        "        # biases initialised as zero\n",
        "        self.b1 = np.zeros((1, h))\n",
        "        self.b2 = np.zeros((1, h))\n",
        "        self.b3 = np.zeros((1, m))\n",
        "\n",
        "  # rectified linear unit activation function\n",
        "  def ReLU(self, r):\n",
        "    return (r>0) * r\n",
        "\n",
        "  # the gradient of the ReLU function\n",
        "  def ReLU_gradient(self, r):\n",
        "    return (r>0)\n",
        "\n",
        "  # hyperbolic tangent activation function\n",
        "  def hTan(self, r):\n",
        "    return np.tanh(r)\n",
        "\n",
        "  # the gradient of the hTan function\n",
        "  def hTan_gradient(self, r):\n",
        "    return 1 - (np.tanh(r) * np.tanh(r))\n",
        "\n",
        "  # mean squared error loss function\n",
        "  def MSE_loss(self, yhat, true_y):\n",
        "    assert yhat.shape == true_y.shape\n",
        "    return 0.5 * np.sum( np.multiply(yhat-true_y,yhat-true_y) ) / yhat.size\n",
        "\n",
        "  # prediction\n",
        "  def forward_pass(self):\n",
        "      self.a1 = np.dot(self.X, self.w1) + self.b1\n",
        "      self.z1 = self.ReLU(self.a1)\n",
        "\n",
        "      self.a2 = np.dot(self.z1, self.w2) + self.b2\n",
        "      self.z2 = self.ReLU(self.a2)\n",
        "\n",
        "      self.a3 = np.dot(self.z2, self.w3) + self.b3\n",
        "      self.z3 = self.hTan(self.a3)  # hTan\n",
        "      # self.z3 = self.a3           # no activation function, i.e. linear(x)=x\n",
        "\n",
        "\n",
        "  # gradient update\n",
        "  def backward_pass(self):\n",
        "\n",
        "      dl_dz3 = self.z3 - self.y                        # (n x m)      \n",
        "      dz3_da3 = self.hTan_gradient(self.a3)            # (n x m) # hTan\n",
        "      delta3 = np.multiply(dl_dz3, dz3_da3)            # (n x m)\n",
        "      # delta3 = dl_dz3                                # (n x m) # no activation function, i.e. linear(x)=x (unstable)\n",
        "      da3_dw3 = self.z2                                # (n x h)\n",
        "      self.w3 -= self.learning_rate * ( np.dot(da3_dw3.T, delta3) + self.momentum * self.w3_stored)     # (h x m)\n",
        "      self.w3_stored = np.dot(da3_dw3.T, delta3) + self.momentum * self.w3_stored\n",
        "      self.b3 -= self.learning_rate * np.sum(delta3, axis=0)                                            # (1 x m)\n",
        "\n",
        "      dl_dz2 = np.dot(delta3, self.w3.T)               # (n x h)\n",
        "      dz2_da2 = self.ReLU_gradient(self.a2)            # (n x h)\n",
        "      delta2 = np.multiply(dl_dz2, dz2_da2)            # (n x h)\n",
        "      da2_dw2 = self.z1                                # (n x h)\n",
        "      self.w2 -= self.learning_rate * (np.dot(da2_dw2.T, delta2) + self.momentum * self.w2_stored)      # (h x h)\n",
        "      self.w2_stored = np.dot(da2_dw2.T, delta2) + self.momentum * self.w2_stored\n",
        "      self.b2 -= self.learning_rate * np.sum(delta2, axis=0)                                            # (1 x h)\n",
        "\n",
        "\n",
        "      dl_dz1 = np.dot(delta2, self.w2.T)                # (n x h)\n",
        "      dz1_da1 = self.ReLU_gradient(self.a1)             # (n x h)\n",
        "      delta1 = np.multiply(dl_dz1, dz1_da1)             # (n x h)\n",
        "      da1_dw1 = self.X                                  # (n x d)\n",
        "      self.w1 -= self.learning_rate * (np.dot(da1_dw1.T, delta1)  + self.momentum * self.w1_stored)     # (d x h)\n",
        "      self.w1_stored = np.dot(da1_dw1.T, delta1)  + self.momentum * self.w1_stored\n",
        "      self.b1 -= self.learning_rate * np.sum(delta1, axis=0)                                            # (1 x h)\n",
        "\n",
        "\n",
        "  def run_prediction(self, data_X):\n",
        "    self.X = data_X\n",
        "    self.forward_pass()\n",
        "    return self.z3\n",
        "\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDJnrC5shHVD"
      },
      "source": [
        "Running the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1vA6Dj47xy2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2837bf52-1f91-4ecf-e079-f9dcf92b4226"
      },
      "source": [
        "def MSE(yhat, true_y):\n",
        "  assert yhat.shape == true_y.shape\n",
        "  return 0.5 * np.sum( np.multiply(yhat-true_y,yhat-true_y) ) / yhat.size\n",
        "\n",
        "def model_evaluation():\n",
        "\n",
        "  validation = []\n",
        "  xv = KFold(n_splits=4, random_state=1, shuffle=True)\n",
        "\n",
        "  for xv_trn, xv_val in xv.split(train):\n",
        "    \n",
        "    trn_dataloader = DataLoader(TensorDataset(Tensor(dataset.X[xv_trn]),Tensor(dataset.y[xv_trn])), batch_size=20, shuffle=True)\n",
        "\n",
        "    epochs = 5\n",
        "    predictions, true_ = [],[]\n",
        "\n",
        "    model = my_MLP(dataset.X[train.indices],dataset.y[train.indices])\n",
        "    \n",
        "    for ep in range(epochs):\n",
        "      for i, (inputs, targets) in enumerate(trn_dataloader):\n",
        "        model.X = inputs.numpy()\n",
        "        model.y = targets.numpy()\n",
        "        model.forward_pass()\n",
        "        model.backward_pass()\n",
        "\n",
        "    for X_val, y_val in zip(dataset.X[xv_val], dataset.y[xv_val]):\n",
        "      yhat = model.run_prediction(X_val).flatten()\n",
        "\n",
        "      predictions.append(yhat)\n",
        "      true_.append(y_val)\n",
        "    \n",
        "    predictions, true_ = np.vstack(predictions), np.vstack(true_)\n",
        "    validation.append(MSE(predictions,true_))\n",
        "  \n",
        "  return validation\n",
        "\n",
        "res = model_evaluation()\n",
        "print(\"Cross-validation errors:\", res, np.mean(res), np.std(res))\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-validation errors: [0.2596883703543092, 0.7354069771861644, 0.25745914708805545, 0.2841913437629887] 0.38418645959787945 0.20304830775401614\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBpiKeng598D"
      },
      "source": [
        "While using ReLU for the outer layer I could not make this work. Once I switched to hTan it seems to work. It is, however, still unclear why the linear activation function (no activation function effectively) creates numerical instability in np.multiply(). I will show the final loss using this implementation, but for the hyperparameter-tuning I resort to using keras Sequential() where the loss function of the different targets (multi-output setup) is the same.\n",
        "\n",
        "A more robust approach would be the following."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndOce5gaXAE0"
      },
      "source": [
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "def MSE(yhat, true_y):\n",
        "  assert yhat.shape == true_y.shape\n",
        "  return 0.5 * np.sum( np.multiply(yhat-true_y,yhat-true_y) ) / yhat.size\n",
        "\n",
        "def keras_MLP(X, y, layers, h, epochs, batch_size, alpha, instances):\n",
        "\n",
        "  model = Sequential()\n",
        "\n",
        "  # input layer and first hidden layer\n",
        "  model.add(Dense(units=h, input_dim=X.shape[1], kernel_initializer='normal', activation='relu'))\n",
        "  # more hidden layers\n",
        "  for iter in range(layers-1):\n",
        "    model.add(Dense(units=h, kernel_initializer='normal', activation='relu'))\n",
        "  # output layer\n",
        "  model.add(Dense(y.shape[1], kernel_initializer='normal', activation=None))\n",
        "\n",
        "  model.compile(loss='mean_squared_error', optimizer=keras.optimizers.Adam(learning_rate=alpha))\n",
        "\n",
        "  count = int(X.shape[0] * instances) # how many data points to use from the available training data (fixed_instances=1 means use all)\n",
        "\n",
        "  model.fit(X[0:count], y[0:count], batch_size = batch_size, epochs = epochs, verbose=0)\n",
        "  return model\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNFOxJ2J7xmQ"
      },
      "source": [
        "Hyperparameter tuning using cross-validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVJgn0RlbLkK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a52a8b1d-8cf7-4e02-9bb0-2142b62483cb"
      },
      "source": [
        "# finds the optminal number of hidden layers, node length (neurons) and batching size\n",
        "def parameter_tuning(layers, hidden_nodes, batch_sizes):\n",
        "\n",
        "  validation = []\n",
        "\n",
        "  k = 2\n",
        "  xv = KFold(n_splits=k, random_state=1, shuffle=True)\n",
        "\n",
        "  for xv_trn, xv_val in xv.split(train):\n",
        "    \n",
        "    X_trn, y_trn = dataset.X[xv_trn], dataset.y[xv_trn]\n",
        "    X_val, y_val = dataset.X[xv_val], dataset.y[xv_val]\n",
        "\n",
        "    fixed_epoch = 50\n",
        "    fixed_instances = 1.0\n",
        "    fixed_learning_rate = 0.01\n",
        "\n",
        "    predictions = np.zeros((len(layers), len(hidden_nodes), batch_sizes.shape[0]))\n",
        "\n",
        "    for layer_idx in range(len(layers)):\n",
        "      for h_idx in range(len(hidden_nodes)):\n",
        "        for b_idx in range(batch_sizes.shape[0]):\n",
        "          \n",
        "          print(\"Train with:\", layers[layer_idx], hidden_nodes[h_idx], fixed_epoch, batch_sizes[b_idx], fixed_learning_rate, fixed_instances)\n",
        "          model=keras_MLP(X_trn, y_trn, layers[layer_idx], hidden_nodes[h_idx], fixed_epoch, batch_sizes[b_idx], fixed_learning_rate, fixed_instances)\n",
        "\n",
        "          pred, true_ = model.predict(X_val), y_val\n",
        "          error = MSE(pred,true_)\n",
        "          print(\"MSE:\", error)\n",
        "\n",
        "          predictions[layer_idx,h_idx,b_idx] = error\n",
        "    \n",
        "    validation.append(predictions)\n",
        "  \n",
        "  validation_table = np.mean(np.array(validation), axis=0)\n",
        "\n",
        "  idx = np.unravel_index(np.argmin(validation_table), validation_table.shape)\n",
        "\n",
        "  return validation_table, layers[idx[0]], hidden_nodes[idx[1]], batch_sizes[idx[2]]\n",
        "\n",
        "\n",
        "\n",
        "layers = [i for i in range(1,6)]                          \n",
        "hidden_nodes = [i for i in range(1,42,5)]                 \n",
        "batch_sizes = np.linspace(5,X.shape[0]/3,10).astype(int)  \n",
        "\n",
        "print(layers,hidden_nodes,batch_sizes)\n",
        "\n",
        "validation_table, layer, h, batch = parameter_tuning(layers, hidden_nodes, batch_sizes)\n",
        "\n",
        "print(layer, h, batch) # 2, 11 and 56\n",
        "\n",
        "layers_error = np.mean(validation_table, axis=(1,2))\n",
        "hidden_nodes_error = np.mean(validation_table, axis=(0,2))\n",
        "batch_sizes_error = np.mean(validation_table, axis=(0,1))\n",
        "\n",
        "# plt.plot(batch_sizes,batch_sizes_error)\n",
        "# plt.ylabel('MSE')\n",
        "# plt.title('Error for batch size')\n",
        "# plt.savefig('batch size error.png')\n",
        "\n",
        "# plt.close()\n",
        "# plt.plot(layers,layers_error)\n",
        "# plt.ylabel('MSE')\n",
        "# plt.title('Error for number of hidden layers')\n",
        "# plt.savefig('layer size error.png')\n",
        "\n",
        "# plt.close()\n",
        "# plt.plot(hidden_nodes,hidden_nodes_error)\n",
        "# plt.ylabel('MSE')\n",
        "# plt.title('Error for number of nodes (neurons)')\n",
        "# plt.savefig('node size error.png')\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3, 4, 5] [1, 6, 11, 16, 21, 26, 31, 36, 41] [  5  56 107 158 210 261 312 364 415 466]\n",
            "Train with: 1 1 50 5 0.01 1.0\n",
            "MSE: 0.0924873890796804\n",
            "Train with: 1 1 50 56 0.01 1.0\n",
            "MSE: 0.09439062607685565\n",
            "Train with: 1 1 50 107 0.01 1.0\n",
            "MSE: 0.09713520379635393\n",
            "Train with: 1 1 50 158 0.01 1.0\n",
            "MSE: 0.10227433673135722\n",
            "Train with: 1 1 50 210 0.01 1.0\n",
            "MSE: 0.11021685225595484\n",
            "Train with: 1 1 50 261 0.01 1.0\n",
            "MSE: 0.11978643987226154\n",
            "Train with: 1 1 50 312 0.01 1.0\n",
            "MSE: 0.12320684585875272\n",
            "Train with: 1 1 50 364 0.01 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored in: <function IteratorResourceDeleter.__del__ at 0x7f80467ef440>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\", line 546, in __del__\n",
            "    handle=self._handle, deleter=self._deleter)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 1264, in delete_iterator\n",
            "    _ctx, \"DeleteIterator\", name, handle, deleter)\n",
            "KeyboardInterrupt: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 0.13888080507932185\n",
            "Train with: 1 1 50 415 0.01 1.0\n",
            "MSE: 0.13177776000804853\n",
            "Train with: 1 1 50 466 0.01 1.0\n",
            "MSE: 0.13228504816142653\n",
            "Train with: 1 6 50 5 0.01 1.0\n",
            "MSE: 0.0003365751255549085\n",
            "Train with: 1 6 50 56 0.01 1.0\n",
            "MSE: 1.6879609737929462e-14\n",
            "Train with: 1 6 50 107 0.01 1.0\n",
            "MSE: 0.0003619032929706974\n",
            "Train with: 1 6 50 158 0.01 1.0\n",
            "MSE: 0.002556460970926512\n",
            "Train with: 1 6 50 210 0.01 1.0\n",
            "MSE: 0.003628603350226214\n",
            "Train with: 1 6 50 261 0.01 1.0\n",
            "MSE: 0.006700866753306402\n",
            "Train with: 1 6 50 312 0.01 1.0\n",
            "MSE: 0.007032247726902424\n",
            "Train with: 1 6 50 364 0.01 1.0\n",
            "MSE: 0.01661412574112119\n",
            "Train with: 1 6 50 415 0.01 1.0\n",
            "MSE: 0.021258940217067503\n",
            "Train with: 1 6 50 466 0.01 1.0\n",
            "MSE: 0.01788348130214713\n",
            "Train with: 1 11 50 5 0.01 1.0\n",
            "MSE: 0.0001836288209221145\n",
            "Train with: 1 11 50 56 0.01 1.0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-1d02b65b747d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhidden_nodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0mvalidation_table\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparameter_tuning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_nodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 2, 11 and 56\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-33-1d02b65b747d>\u001b[0m in \u001b[0;36mparameter_tuning\u001b[0;34m(layers, hidden_nodes, batch_sizes)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Train with:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_nodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mh_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfixed_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfixed_learning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfixed_instances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeras_MLP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_trn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_trn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_nodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mh_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfixed_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfixed_learning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfixed_instances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m           \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-8d712126655c>\u001b[0m in \u001b[0;36mkeras_MLP\u001b[0;34m(X, y, layers, h, epochs, batch_size, alpha, instances)\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0minstances\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# how many data points to use from the available training data (fixed_instances=1 means use all)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3039\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3040\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3042\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1964\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1966\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 921
        },
        "id": "gRLf95-WofH1",
        "outputId": "d3fd212c-fcd7-4dab-b276-a44ce527028a"
      },
      "source": [
        "# finds the optminal number of epochs and evaluates the model using bias-variance decomposition on the validation set\n",
        "def bias_variance_evaluation(epochs, instances):\n",
        "\n",
        "  validation, validation_trn = [], []\n",
        "\n",
        "  k = 2\n",
        "  xv = KFold(n_splits=k, random_state=1, shuffle=True)\n",
        "\n",
        "  for xv_trn, xv_val in xv.split(train):\n",
        "    \n",
        "    X_trn, y_trn = dataset.X[xv_trn], dataset.y[xv_trn]\n",
        "    X_val, y_val = dataset.X[xv_val], dataset.y[xv_val]\n",
        "\n",
        "    # from parameter_tuning(layers, hidden_nodes, batch_sizes)\n",
        "    fixed_layer = 2\n",
        "    fixed_nodes = 11\n",
        "    fixed_batch = 56\n",
        "    fixed_learning_rate = 0.01\n",
        "\n",
        "    predictions = np.zeros((len(epochs), len(instances)))\n",
        "    predictions_trn = np.zeros((len(epochs), len(instances)))\n",
        "\n",
        "\n",
        "    for epoch_idx in range(len(epochs)):\n",
        "      for instance_idx in range(len(instances)):\n",
        "          \n",
        "          print(\"Train with:\", fixed_layer, fixed_nodes, epochs[epoch_idx], fixed_batch, fixed_learning_rate, instances[instance_idx])\n",
        "          model=keras_MLP(X, y, fixed_layer, fixed_nodes, epochs[epoch_idx], fixed_batch, fixed_learning_rate, instances[instance_idx])\n",
        "\n",
        "          pred, true_ = model.predict(X_val), y_val\n",
        "          error = MSE(pred,true_)\n",
        "          print(\"MSE:\", error)\n",
        "          predictions[epoch_idx,instance_idx] = error\n",
        "\n",
        "          pred_trn, true_trn = model.predict(X_trn), y_trn\n",
        "          error_trn = MSE(pred_trn,true_trn)\n",
        "          print(\"MSE:\", error_trn)\n",
        "          predictions_trn[epoch_idx,instance_idx] = error_trn\n",
        "\n",
        "    validation.append(predictions)\n",
        "    validation_trn.append(predictions_trn)    \n",
        "\n",
        "\n",
        "  validation_table = np.mean(np.array(validation), axis=0)          # validation\n",
        "  validation_table_trn = np.mean(np.array(validation_trn), axis=0)  # training\n",
        "\n",
        "  idx = np.unravel_index(np.argmin(validation_table), validation_table.shape)\n",
        "\n",
        "  return validation_table, validation_table_trn, epochs[idx[0]], instances[idx[1]]\n",
        "\n",
        "\n",
        "epochs = [i for i in range(5,505,50)]     \n",
        "instances = [i/10 for i in range(1,11)]\n",
        "\n",
        "print(epochs, instances)\n",
        "\n",
        "validation_table, validation_table_trn, epoch, instance = bias_variance_evaluation(epochs, instances)\n",
        "\n",
        "print(epoch, instance) # 205 and 0.8\n",
        "\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5, 55, 105, 155, 205, 255, 305, 355, 405, 455] [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
            "Train with: 2 11 5 56 0.01 0.1\n",
            "MSE: 0.34136823052715576\n",
            "MSE: 0.3274394642219167\n",
            "Train with: 2 11 5 56 0.01 0.2\n",
            "MSE: 0.13488550073627947\n",
            "MSE: 0.1269244999085841\n",
            "Train with: 2 11 5 56 0.01 0.3\n",
            "MSE: 0.09390671715498887\n",
            "MSE: 0.08760532504309818\n",
            "Train with: 2 11 5 56 0.01 0.4\n",
            "MSE: 0.032767497633559364\n",
            "MSE: 0.032428583848356334\n",
            "Train with: 2 11 5 56 0.01 0.5\n",
            "MSE: 0.02520998079605384\n",
            "MSE: 0.02469337464352382\n",
            "Train with: 2 11 5 56 0.01 0.6\n",
            "MSE: 0.023589683598624595\n",
            "MSE: 0.0229003205167667\n",
            "Train with: 2 11 5 56 0.01 0.7\n",
            "MSE: 0.015182146664723763\n",
            "MSE: 0.0148338082113493\n",
            "Train with: 2 11 5 56 0.01 0.8\n",
            "MSE: 0.018301965349652012\n",
            "MSE: 0.0177126196107089\n",
            "Train with: 2 11 5 56 0.01 0.9\n",
            "MSE: 0.017173481391766724\n",
            "MSE: 0.01620782772995405\n",
            "Train with: 2 11 5 56 0.01 1.0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-cda68f3aaa83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m \u001b[0mvalidation_table\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_table_trn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbias_variance_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 205 and 0.8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-cda68f3aaa83>\u001b[0m in \u001b[0;36mbias_variance_evaluation\u001b[0;34m(epochs, instances)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Train with:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfixed_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfixed_nodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mepoch_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfixed_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfixed_learning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstances\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minstance_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeras_MLP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfixed_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfixed_nodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mepoch_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfixed_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfixed_learning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstances\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minstance_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m           \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-8d712126655c>\u001b[0m in \u001b[0;36mkeras_MLP\u001b[0;34m(X, y, layers, h, epochs, batch_size, alpha, instances)\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0minstances\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# how many data points to use from the available training data (fixed_instances=1 means use all)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3039\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3040\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3042\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1964\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1966\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0eFU6YcwIAiA",
        "outputId": "d42d6bf1-ff5c-47ab-c57f-f94cf533f889"
      },
      "source": [
        "# finds the optminal number of epochs and evaluates the model using bias-variance decomposition on the test set\n",
        "def bias_variance_evaluation_test_set(epochs, instances):\n",
        "\n",
        "  validation, validation_trn = [], []\n",
        "\n",
        "  k = 2\n",
        "  xv = KFold(n_splits=k, random_state=1, shuffle=True)\n",
        "\n",
        "  for xv_trn, xv_val in xv.split(train):\n",
        "    \n",
        "    X_trn, y_trn = dataset.X[xv_trn], dataset.y[xv_trn]\n",
        "    X_val, y_val = dataset.X[xv_val], dataset.y[xv_val]\n",
        "    X_tst, y_tst = dataset.X[test.indices], dataset.y[test.indices]\n",
        "\n",
        "    # from parameter_tuning(layers, hidden_nodes, batch_sizes)\n",
        "    fixed_layer = 2\n",
        "    fixed_nodes = 11\n",
        "    fixed_batch = 56\n",
        "    fixed_learning_rate = 0.01\n",
        "\n",
        "    predictions = np.zeros((len(epochs), len(instances)))\n",
        "    predictions_trn = np.zeros((len(epochs), len(instances)))\n",
        "\n",
        "\n",
        "    for epoch_idx in range(len(epochs)):\n",
        "      for instance_idx in range(len(instances)):\n",
        "          \n",
        "          print(\"Train with:\", fixed_layer, fixed_nodes, epochs[epoch_idx], fixed_batch, fixed_learning_rate, instances[instance_idx])\n",
        "\n",
        "          model=keras_MLP(X, y, fixed_layer, fixed_nodes, epochs[epoch_idx], fixed_batch, fixed_learning_rate, instances[instance_idx])\n",
        "\n",
        "          pred, true_ = model.predict(X_tst), y_tst\n",
        "          error = MSE(pred,true_)\n",
        "          print(\"MSE:\", error)\n",
        "          predictions[epoch_idx,instance_idx] = error\n",
        "\n",
        "          pred_trn, true_trn = model.predict(X_trn), y_trn\n",
        "          error_trn = MSE(pred_trn,true_trn)\n",
        "          print(\"MSE:\", error_trn)\n",
        "          predictions_trn[epoch_idx,instance_idx] = error_trn\n",
        "\n",
        "\n",
        "    validation.append(predictions )         #test\n",
        "    validation_trn.append(predictions_trn)  #training\n",
        "\n",
        "\n",
        "  validation_table = np.mean(np.array(validation), axis=0)\n",
        "  validation_table_trn = np.mean(np.array(validation_trn), axis=0)\n",
        "\n",
        "  idx = np.unravel_index(np.argmin(validation_table), validation_table.shape)\n",
        "\n",
        "  return validation_table, validation_table_trn, epochs[idx[0]], instances[idx[1]]\n",
        "\n",
        "\n",
        "epochs = [i for i in range(5,505,50)]     \n",
        "instances = [i/10 for i in range(1,11)]\n",
        "\n",
        "validation_table_tst, validation_table_trn, epoch, instance = bias_variance_evaluation_test_set(epochs, instances)\n",
        "\n",
        "print(epoch, instance) # 55 and 1 (which is different from 205 and 0.8 earlier)\n",
        "\n",
        "# using instances=0.8 only\n",
        "epochs_error_trn = validation_table_trn[:,7] # for 0.8\n",
        "epochs_error_trn = validation_table_tst[:,7]\n",
        "\n",
        "# using epochs=205 only\n",
        "instances_error_tst = validation_table_tst[4,:] # for 205\n",
        "instances_error_trn = validation_table_trn[4,:]\n",
        "\n",
        "\n",
        "plt.plot(epochs, epochs_error_tst, label='test')\n",
        "plt.plot(epochs, epochs_error_trn, label='training')\n",
        "plt.ylabel('MSE')\n",
        "plt.legend()\n",
        "plt.title('Error for epoch size')\n",
        "plt.savefig('epoch size error test.png')\n",
        "\n",
        "# plt.close()\n",
        "# plt.plot(instances, instances_error_tst, label='test')\n",
        "# plt.plot(instances, instances_error_trn, label='training')\n",
        "# plt.ylabel('MSE')\n",
        "# plt.legend()\n",
        "# plt.title('Error for sample size')\n",
        "# plt.savefig('sample size error test.png')\n",
        "\n",
        "\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train with: 2 11 5 56 0.01 0.1\n",
            "MSE: 0.3197087217584346\n",
            "MSE: 0.31856425220584395\n",
            "Train with: 2 11 5 56 0.01 0.2\n",
            "MSE: 0.12088488267311957\n",
            "MSE: 0.12142022118199593\n",
            "Train with: 2 11 5 56 0.01 0.3\n",
            "MSE: 0.05781628004129744\n",
            "MSE: 0.0553832455486037\n",
            "Train with: 2 11 5 56 0.01 0.4\n",
            "MSE: 0.09103430808336542\n",
            "MSE: 0.09015837322443604\n",
            "Train with: 2 11 5 56 0.01 0.5\n",
            "MSE: 0.026460981052426377\n",
            "MSE: 0.02540670640471068\n",
            "Train with: 2 11 5 56 0.01 0.6\n",
            "MSE: 0.021809154093018833\n",
            "MSE: 0.02066126320987134\n",
            "Train with: 2 11 5 56 0.01 0.7\n",
            "MSE: 0.021547556041219478\n",
            "MSE: 0.02082014355279075\n",
            "Train with: 2 11 5 56 0.01 0.8\n",
            "MSE: 0.010464554981685054\n",
            "MSE: 0.010613499488487986\n",
            "Train with: 2 11 5 56 0.01 0.9\n",
            "MSE: 0.012443208216806625\n",
            "MSE: 0.012167187857014445\n",
            "Train with: 2 11 5 56 0.01 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored in: <function IteratorResourceDeleter.__del__ at 0x7f80467ef440>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\", line 546, in __del__\n",
            "    handle=self._handle, deleter=self._deleter)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 1264, in delete_iterator\n",
            "    _ctx, \"DeleteIterator\", name, handle, deleter)\n",
            "KeyboardInterrupt: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 0.006505822641258127\n",
            "MSE: 0.005952251243698336\n",
            "Train with: 2 11 55 56 0.01 0.1\n",
            "MSE: 0.005411607007081888\n",
            "MSE: 0.005503600736898596\n",
            "Train with: 2 11 55 56 0.01 0.2\n",
            "MSE: 0.0020834779959083565\n",
            "MSE: 0.002023130521265229\n",
            "Train with: 2 11 55 56 0.01 0.3\n",
            "MSE: 0.00013845624596841834\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored in: <function IteratorResourceDeleter.__del__ at 0x7f80467ef440>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\", line 546, in __del__\n",
            "    handle=self._handle, deleter=self._deleter)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 1264, in delete_iterator\n",
            "    _ctx, \"DeleteIterator\", name, handle, deleter)\n",
            "KeyboardInterrupt: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 0.0001567141038260259\n",
            "Train with: 2 11 55 56 0.01 0.4\n",
            "MSE: 0.000160819507436753\n",
            "MSE: 0.00016622776659735148\n",
            "Train with: 2 11 55 56 0.01 0.5\n",
            "MSE: 9.943216191411394e-06\n",
            "MSE: 7.711185624892483e-06\n",
            "Train with: 2 11 55 56 0.01 0.6\n",
            "MSE: 2.6666334645708566e-05\n",
            "MSE: 2.4709561946429826e-05\n",
            "Train with: 2 11 55 56 0.01 0.7\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-24383a243307>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0minstances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m \u001b[0mvalidation_table_tst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_table_trn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbias_variance_evaluation_test_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 55 and 1 (which is different from 205 and 0.8 earlier)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-35-24383a243307>\u001b[0m in \u001b[0;36mbias_variance_evaluation_test_set\u001b[0;34m(epochs, instances)\u001b[0m\n\u001b[1;32m     28\u001b[0m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Train with:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfixed_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfixed_nodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mepoch_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfixed_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfixed_learning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstances\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minstance_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeras_MLP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfixed_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfixed_nodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mepoch_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfixed_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfixed_learning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstances\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minstance_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m           \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-8d712126655c>\u001b[0m in \u001b[0;36mkeras_MLP\u001b[0;34m(X, y, layers, h, epochs, batch_size, alpha, instances)\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0minstances\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# how many data points to use from the available training data (fixed_instances=1 means use all)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3039\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3040\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3042\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1964\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1966\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Wc84o93bLAn"
      },
      "source": [
        "Loss on the test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJU16DnD0wja"
      },
      "source": [
        "# optimal parameters\n",
        "layer = 2\n",
        "nodes = 11\n",
        "batch = 56\n",
        "learning_rate = 0.01\n",
        "epochs = 205\n",
        "X, y = dataset.X[train.indices], dataset.y[train.indices]\n",
        "\n",
        "model=keras_MLP(X, y, layer, nodes, epochs, batch, learning_rate, 1.0)\n",
        "predictions = model.predict(dataset.X[test.indices])\n",
        "true_ = dataset.y[test.indices]\n",
        "print(\"Final error on the test set:\", MSE(predictions,true_))\n",
        "\n",
        "\n",
        "trn_dataloader = DataLoader(TensorDataset(Tensor(X),Tensor(y)), batch_size=batch, shuffle=True)\n",
        "model = my_MLP(X,y)\n",
        "\n",
        "for ep in range(epochs):\n",
        "  for i, (inputs, targets) in enumerate(trn_dataloader):\n",
        "    model.X = inputs.numpy()\n",
        "    model.y = targets.numpy()\n",
        "    model.forward_pass()\n",
        "    model.backward_pass()\n",
        "\n",
        "predictions=[]\n",
        "for X_tst in dataset.X[test.indices]:\n",
        "  yhat = model.run_prediction(X_tst).flatten()\n",
        "  predictions.append(yhat)\n",
        "\n",
        "predictions = np.vstack(predictions)\n",
        "print(\"Final error on the test set:\", MSE(predictions,true_))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}